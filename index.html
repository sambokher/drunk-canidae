<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EvalKit | Model Evaluation Platform</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <header>
        <nav class="container">
            <div class="logo">EvalKit</div>
            <div class="nav-links">
                <a href="#">Docs</a>
                <a href="#">Benchmarks</a>
                <a href="#" class="nav-cta">Get Started</a>
            </div>
        </nav>
    </header>

    <main>
        <section class="hero">
            <div class="container">
                <span class="label">Open Source</span>
                <h1>Evaluate your AI models with <span class="code">confidence</span>.</h1>
                <p class="hero-subtext">A comprehensive evaluation framework for testing LLMs, measuring performance, and benchmarking against industry standards.</p>
                <div class="hero-actions">
                    <a href="#" class="btn btn-dark">pip install evalkit</a>
                    <a href="#" class="btn btn-outline">View Docs</a>
                </div>
            </div>
        </section>

        <section class="features">
            <div class="container grid">
                <div class="feature-card">
                    <h3>Automated Testing</h3>
                    <p>Run comprehensive eval suites across multiple models. Define custom metrics and track performance over time with our Python SDK.</p>
                </div>
                <div class="feature-card">
                    <h3>Benchmarking</h3>
                    <p>Compare your models against industry-standard datasets like MMLU, HumanEval, and custom evaluation sets.</p>
                </div>
                <div class="feature-card">
                    <h3>CI/CD Integration</h3>
                    <p>Integrate evals into your workflow with GitHub Actions, pre-commit hooks, and continuous monitoring dashboards.</p>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2024 EvalKit. MIT Licensed. Built with ❤️ for the AI community.</p>
        </div>
    </footer>
</body>
</html>